# --- KRAFT CONTROLLER NODE POOL ---
# Definisce il pool di nodi dedicati al ruolo di "Controller" (gestione metadati).
# Sostituisce ZooKeeper nell'architettura moderna di Kafka (KRaft mode).
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: controller
  namespace: kafka
  labels:
    strimzi.io/cluster: iot-sensor-cluster
spec:
  # High Availability: Avere 2+ controller assicura che il quorum (raft) sopravviva a un guasto.
  # Nota: In produzione ideale dispari (es. 3) per evitare split-brain, ma 2 ok per lab.
  replicas: 2 
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        kraftMetadata: shared
        deleteClaim: false
---
# --- BROKER NODE POOL ---
# Definisce i nodi "Data Plane" che gestiscono i messaggi (Topic, Partizioni).
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: broker
  namespace: kafka
  labels:
    strimzi.io/cluster: iot-sensor-cluster
spec:
  #Scalabilità: Qui definiamo quanti broker gestiscono il traffico dati.
  replicas: 2 
  roles:
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim  # Storage persistente (PVC) per non perdere i messaggi al riavvio
        size: 5Gi
        kraftMetadata: shared
        deleteClaim: false
---
# --- KAFKA CLUSTER DEFINITION ---
# Risorsa principale che orchestra i pool definiti sopra.
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: iot-sensor-cluster
  namespace: kafka
spec:
  kafka:
    version: 4.1.0
    metadataVersion: 4.1-IV1

    # Configurazione Listener (Porte di ascolto)
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
        authentication:
          type: scram-sha-512

    # Configurazione Server Kafka (server.properties)
    config:
      # --- REPLICAZIONE (Data Redundancy) ---
      # Impostiamo il fattore a 2 perché abbiamo definito 2 repliche nel NodePool 'broker'.
      # Ogni messaggio verrà salvato su entrambi i nodi per ridondanza.
      offsets.topic.replication.factor: 2
      transaction.state.log.replication.factor: 2
      default.replication.factor: 2 # Fattore di replicazione per i nuovi topic
      
      # --- CONSISTENZA vs DISPONIBILITÀ (min.isr) ---
      # Definisce quante repliche devono confermare la scrittura affinché sia considerata valida.
      
      # CONFIGURAZIONE TEST (Minikube/Risorse Limitate):
      # Impostiamo a 1. In questo ambiente con risorse limitate (CPU/RAM), diamo priorità 
      # alla disponibilità: se un broker va in crash o è lento, il cluster deve continuare a funzionare.
      
      # CONFIGURAZIONE PRODUZIONE (Best Practice):
      # In un ambiente reale, questo valore andrebbe impostato a 2 (con Replication Factor >= 3).
      # Questo garantirebbe che nessuna scrittura venga persa anche in caso di fallimento di un nodo,offrendo tolleranza ai guasti reale (N-1) senza compromettere la consistenza.
      transaction.state.log.min.isr: 1 
      min.insync.replicas: 1
  
  # Entity Operator: Gestisce Topic e Utenti come risorse K8s (GitOps)
  entityOperator:
    topicOperator: {}
    userOperator: {}
