
===== Cartella Consumer =====
--- Dockerfile ---
FROM python:3.11-slim

WORKDIR /app
COPY consumer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "consumer.py"]

------------------
--- consumer.py ---
from kafka import KafkaConsumer
from pymongo import MongoClient
from datetime import datetime
import json, os

# Configurazione Kafka
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
KAFKA_CA = "/etc/ssl/certs/kafka/ca.crt"

TOPIC = "sensor-stream"

# Configurazione MongoDB (Il Secret punta già a iot_network, ma esplicitiamo il db)
MONGO_URI = os.environ["MONGO_URI"]
client = MongoClient(MONGO_URI)
db = client.iot_network  
collection = db.sensor_data

def handle_boot(event):
    print(f"[BOOT] Device {event.get('device_id')} attivo in zona {event.get('zone_id')}.", flush=True)
    event["_ingest_ts"] = datetime.utcnow()
    collection.insert_one(event)

def handle_telemetry(event):
    print(f"[TELEMETRY] Device {event.get('device_id')} -> Temp: {event.get('temperature')}°C, Hum: {event.get('humidity')}%", flush=True)
    event["_ingest_ts"] = datetime.utcnow()
    collection.insert_one(event)

def handle_firmware_update(event):
    print(f"[UPDATE] Device {event.get('device_id')} aggiornamento a {event.get('update_to')}.", flush=True)
    event["_ingest_ts"] = datetime.utcnow()
    collection.insert_one(event)

def handle_alert(event):
    print(f"[ALERT] CRITICO: Device {event.get('device_id')} Code: {event.get('error_code')}", flush=True)
    event["_ingest_ts"] = datetime.utcnow()
    collection.insert_one(event)

def handle_unknown(event):
    print(f"[IGNOTO] Tipo evento non gestito: {event.get('type')}", flush=True)
    event["_ingest_ts"] = datetime.utcnow()
    collection.insert_one(event)

# Inizializzazione Consumer
consumer = KafkaConsumer(
    TOPIC,
    bootstrap_servers=KAFKA_BOOTSTRAP,
    security_protocol="SASL_SSL",
    sasl_mechanism="SCRAM-SHA-512",
    sasl_plain_username=SASL_USERNAME,
    sasl_plain_password=SASL_PASSWORD,
    ssl_cafile=KAFKA_CA,
    auto_offset_reset='earliest', # Legge dall'inizio 
    group_id='iot-consumer-group',
    value_deserializer=lambda v: json.loads(v.decode('utf-8'))
)

print(f"IoT Consumer avviato. In ascolto su: {TOPIC}", flush=True)

# Loop principale
for message in consumer:
    event = message.value
    
    event_type = event.get("type")
    match event_type:
        case "boot":
            handle_boot(event)
        case "telemetry":
            handle_telemetry(event)
        case "firmware_update":
            handle_firmware_update(event)
        case "alert":
            handle_alert(event)
        case _:
            handle_unknown(event)
-------------------
--- requirements.txt ---
kafka-python
pymongo
------------------------
===== FINE Cartella Consumer =====

===== Cartella K8s =====
--- consumer-deployment.yaml ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consumer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consumer
  template:
    metadata:
      labels:
        app: consumer
    spec:
      containers:
      - name: consumer
        image: consumer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "consumer-user"   # <── Username fisso
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: consumer-user
              key: password         # <── solo password dal Secret
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
          #value: "mongodb://appuser:appuserpass@mongo-mongodb.kafka.svc.cluster.local:27017/student_events?authSource=student_events"
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert

--------------------------------
--- producer-ingress.yaml ---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: producer-ingress
  namespace: kafka
  annotations:
    konghq.com/plugins: jwt-auth  #ABILITA AUTENTICAZIONE CON JWT
    #konghq.com/strip-path: "true"
spec:
  ingressClassName: kong  
  rules:
  - host: producer.192.168.58.2.nip.io
    http:
      paths:
        - path: /event
          pathType: Prefix
          backend:
            service:
              name: producer-service
              port:
                number: 5000

-----------------------------
--- metrics-ingress.yaml ---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: metrics-ingress
  namespace: metrics
  annotations:
    konghq.com/plugins: jwt-auth  #ABILITA AUTENTICAZIONE CON JWT
    #konghq.com/strip-path: "true"
spec:
  ingressClassName: kong
  rules:
    - host: metrics.192.168.58.2.nip.io
      http:
        paths:
          - path: /metrics
            pathType: Prefix
            backend:
              service:
                name: metrics-service
                port:
                  number: 5001
----------------------------
--- jwt-plugin-kafka.yaml ---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: kafka
plugin: jwt
config:
  claims_to_verify:
    - exp

-----------------------------
--- jwt-plugin-metrics.yaml ---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: metrics
plugin: jwt
config:
  claims_to_verify:
    - exp

-------------------------------
--- jwt-credential.yaml ---
apiVersion: v1
kind: Secret
metadata:
  name: exam-client-jwt
  namespace: kafka
  labels:
    konghq.com/credential: jwt    # Label fondamentale per il discovery di Kong
stringData:
  key: "exam-client-key"  #valore che useremo come iss nel token;
  secret: "supersecret"   #chiave condivisa per firmare/verificare il token
  algorithm: "HS256"


---------------------------
--- kafka-cluster.yaml ---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: controller
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  replicas: 2 #per HA Assicura che i metadati del cluster (gestiti da Kraft) siano disponibili anche se 1 controller su 3 fallisce.
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        kraftMetadata: shared
        deleteClaim: false
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: broker
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  replicas: 2 #per HA Consente ai client di connettersi ad altri broker se uno fallisce.
  roles:
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        kraftMetadata: shared
        deleteClaim: false
---
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: uni-it-cluster
  namespace: kafka
spec:
  kafka:
    version: 4.1.0
    metadataVersion: 4.1-IV1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
        authentication:
          type: scram-sha-512
    config:
    #Garantisce che 3 copie di ogni partizione esistano e che almeno 2 siano sincronizzate. Se 1 broker fallisce, la scrittura può continuare verso le 2 rimanenti senza perdere dati.
      # Aumenta il fattore di replicazione a 3
      offsets.topic.replication.factor: 2
      transaction.state.log.replication.factor: 2
      default.replication.factor: 2 # Fattore di replicazione per i nuovi topic
      
      # Minimo di In-Sync Replicas (HA/Durabilità)
      # Il valore di 2 significa che 1 replica può fallire e il sistema resta disponibile (3-1=2)
      transaction.state.log.min.isr: 1 
      min.insync.replicas: 1
  entityOperator:
    topicOperator: {}
    userOperator: {}

--------------------------
--- hpa.yaml ---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: producer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: producer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50   # scala sopra il 50% di CPU media

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: consumer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: consumer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-service-hpa
  namespace: metrics
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics-service
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

----------------
--- kafka-users.yaml ---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: producer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: consumer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
------------------------
--- metrics-deployment.yaml ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-service
  namespace: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics-service
  template:
    metadata:
      labels:
        app: metrics-service
    spec:
      containers:
      - name: metrics-service
        image: metrics-service:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
          #value: "mongodb://appuser:appuserpass@mongo-mongodb.kafka.svc.cluster.local:27017/student_events?authSource=student_events"
        ports:
        - containerPort: 5001
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"

---
apiVersion: v1
kind: Service
metadata:
  name: metrics-service
  namespace: metrics
spec:
  selector:
    app: metrics-service
  ports:
    - protocol: TCP
      port: 5001
      targetPort: 5001
  type: ClusterIP

-------------------------------
--- producer-deployment.yaml ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: producer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: producer
  template:
    metadata:
      labels:
        app: producer
    spec:
      containers:
      - name: producer
        image: producer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "producer-user"   # <── Username fisso
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: producer-user
              key: password         # <── solo password dal Secret
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        ports:
        - containerPort: 5000
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert
---
apiVersion: v1
kind: Service
metadata:
  name: producer-service
  namespace: kafka
spec:
  selector:
    app: producer
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
  type: ClusterIP

--------------------------------
--- kafka-topic.yaml ---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: sensor-stream
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  partitions: 3
  replicas: 1
  config:
    retention.ms: 604800000   # 7 giorni
    segment.bytes: 1073741824 # 1GB per segment
------------------------
--- jwt-consumer.yaml ---
apiVersion: configuration.konghq.com/v1
kind: KongConsumer
metadata:
  name: exam-client
  namespace: kafka
  annotations:
    kubernetes.io/ingress.class: kong
username: exam-client
credentials:
  - exam-client-jwt
-------------------------
===== FINE Cartella K8s =====

===== Cartella Metrics-service =====
--- metrics_service.py ---
from flask import Flask, jsonify
from pymongo import MongoClient
from datetime import datetime, timedelta
import os

app = Flask(__name__)

MONGO_URI = os.environ["MONGO_URI"]
client = MongoClient(MONGO_URI)
db = client.iot_network       
collection = db.sensor_data   

# Test di connessione
try:
    client.admin.command('ping')
    print("Connected to MongoDB (IoT Network)")
except Exception as e:
    print(f"MongoDB connection error: {e}")

# 1. Totale Avvii (Boot Events)
@app.route("/metrics/boots", methods=["GET"])
def total_boots():
    count = collection.count_documents({"type": "boot"})
    return jsonify({"total_device_boots": count})

# 2. Media Temperatura per Zona
@app.route("/metrics/temperature/average-by-zone", methods=["GET"])
def avg_temp_per_zone():
    pipeline = [
        {"$match": {"type": "telemetry"}},
        {"$group": {
            "_id": "$zone_id", 
            "avg_temp": {"$avg": "$temperature"},
            "avg_hum": {"$avg": "$humidity"},
            "samples": {"$sum": 1}
        }}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify(result)

# 3. Conta Allarmi Critici
@app.route("/metrics/alerts", methods=["GET"])
def critical_alerts():
    pipeline = [
        {"$match": {"type": "alert"}},
        {"$group": {"_id": "$severity", "count": {"$sum": 1}}}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify(result)

# 4. Trend Attività ultimi 7 giorni
@app.route("/metrics/activity/last7days", methods=["GET"])
def activity_trend():
    since = datetime.utcnow() - timedelta(days=7)
    pipeline = [
        {"$match": {"_ingest_ts": {"$gte": since}}},
        {"$group": {
            "_id": {"$dateToString": {"format": "%Y-%m-%d", "date": "$_ingest_ts"}}, 
            "events_count": {"$sum": 1}
        }},
        {"$sort": {"_id": 1}}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify(result)

# 5. Stato Firmware
@app.route("/metrics/firmware", methods=["GET"])
def firmware_stats():
    pipeline = [
        {"$match": {"type": "firmware_update"}},
        {"$group": {"_id": "$update_to", "count": {"$sum": 1}}}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify(result)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)
--------------------------
--- Dockerfile ---
FROM python:3.11-slim

WORKDIR /app

COPY metrics_service.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "metrics_service.py"]

------------------
--- requirements.txt ---
flask
pymongo
------------------------
===== FINE Cartella Metrics-service =====

===== Cartella Producer =====
--- Dockerfile ---
FROM python:3.11-slim

WORKDIR /app
COPY producer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 5000

CMD ["python", "producer.py"]

------------------
--- requirements.txt ---
kafka-python
flask
------------------------
--- producer.py ---
from flask import Flask, request, jsonify
from kafka import KafkaProducer
import json, os, uuid
from datetime import datetime

app = Flask(__name__)

# Configurazione Kafka
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")

TOPIC = "sensor-stream"

# Kafka Secure Producer
producer = KafkaProducer(
    bootstrap_servers=KAFKA_BOOTSTRAP,
    security_protocol="SASL_SSL",
    sasl_mechanism="SCRAM-SHA-512",
    sasl_plain_username=SASL_USERNAME,
    sasl_plain_password=SASL_PASSWORD,
    ssl_cafile=KAFKA_CA,
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

# Contatori Locali
event_count = {
    "boot": 0,
    "telemetry": 0,
    "firmware_update": 0,
    "alert": 0
}

def produce_event(event_type: str, payload: dict):
    """Invia evento IoT a Kafka"""
    event = {
        "event_id": str(uuid.uuid4()),
        "type": event_type,
        "timestamp": datetime.utcnow().isoformat(),
        **payload
    }
    producer.send(TOPIC, value=event)
    producer.flush()
    
    # Aggiorna metriche locali se il tipo è noto
    if event_type in event_count:
        event_count[event_type] += 1
        
    print(f"[PRODUCER-IOT] Evento inviato: {event_type} -> {event}", flush=True)
    return event


# 1. Device Boot
@app.route("/event/boot", methods=["POST"])
def device_boot():
    data = request.json or {}
    # Richiede device_id e zone_id (ex course_id)
    required = ["device_id", "zone_id"] 
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing fields: {required}"}), 400

    event = produce_event("boot", {
        "device_id": data["device_id"],
        "zone_id": data["zone_id"]
    })
    return jsonify({"status": "boot_recorded", "event": event}), 200

# 2. Telemetry Data
@app.route("/event/telemetry", methods=["POST"])
def telemetry():
    data = request.json or {}
    # Richiede temperatura e umidità invece di score
    required = ["device_id", "temperature", "humidity", "zone_id"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing fields: {required}"}), 400

    event = produce_event("telemetry", {
        "device_id": data["device_id"],
        "zone_id": data["zone_id"],
        "temperature": float(data["temperature"]),
        "humidity": float(data["humidity"])
    })
    return jsonify({"status": "data_received", "event": event}), 200

# 3. Firmware Update
@app.route("/event/firmware_update", methods=["POST"])
def firmware_update():
    data = request.json or {}
    required = ["device_id", "version_to"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing fields: {required}"}), 400

    event = produce_event("firmware_update", {
        "device_id": data["device_id"],
        "update_to": data["version_to"]
    })
    return jsonify({"status": "update_started", "event": event}), 200

# 4. Critical Alert
@app.route("/event/alert", methods=["POST"])
def alert():
    data = request.json or {}
    required = ["device_id", "error_code", "severity"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing fields: {required}"}), 400

    event = produce_event("alert", {
        "device_id": data["device_id"],
        "error_code": data["error_code"],
        "severity": data["severity"]
    })
    return jsonify({"status": "alert_logged", "event": event}), 200

# Metriche interne
@app.route("/metrics", methods=["GET"])
def metrics():
    total = sum(event_count.values())
    return jsonify({"iot_events_sent": total, "breakdown": event_count}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
-------------------
===== FINE Cartella Producer =====
